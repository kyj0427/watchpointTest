{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMH4meWecUc/hxGSpuzCJA0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PJAkSfY2Evbc"},"outputs":[],"source":["# =======================================================\n","# 1.1: GPU 확인\n","# =======================================================\n","!nvidia-smi"]},{"cell_type":"code","source":["# =======================================================\n","# 1.2: 필수 라이브러리 설치\n","# =======================================================\n","!pip install -q -U transformers peft accelerate bitsandbytes trl pandas huggingface_hub"],"metadata":{"id":"9CF43ZrlFlpr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =======================================================\n","# 1.3: Hugging Face 및 Google Drive 로그인\n","# =======================================================\n","from huggingface_hub import login\n","from google.colab import drive\n","import os\n","\n","login(\n","  token=\"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\"\n",")\n","\n","drive.mount('/content/drive')"],"metadata":{"id":"ykoymgpjFmHJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =======================================================\n","# 2.1: 데이터셋 로딩 및 형식 변환\n","# =======================================================\n","import pandas as pd\n","\n","csv_file_path = '/content/drive/folders/1gqmFCHA8x4Fk-ssQ0x7y9RD-jzcRlGmf?dmr=1&ec=wgc-drive-hero-goto/LLM TEST.csv'\n","\n","# CSV 파일을 Pandas DataFrame으로 불러옵니다.\n","df = pd.read_csv(csv_file_path)\n","\n","# 데이터셋을 Gemma의 프롬프트 템플릿에 맞는 형식으로 변환합니다.\n","# 이 코드가 스프레드시트의 각 행을 파인튜닝용 대화 형식으로 자동 변환합니다.\n","def create_prompt(row):\n","\n","    # \"## 분석 데이터:\"를 기준으로 나눕니다.\n","    parts = row['Instruction'].split('## 분석 데이터:')\n","    system_prompt = parts[0].strip()\n","    user_content = \"## 분석 데이터:\" + parts[1].strip()\n","\n","    # 최종 결과물 (Golden_Response)\n","    assistant_content = row['Golden_Response']\n","\n","    # Gemma-2의 공식 ChatML 템플릿\n","    prompt = f\"<start_of_turn>user\\n{system_prompt}\\n\\n{user_content}<end_of_turn>\\n<start_of_turn>model\\n{assistant_content}<end_of_turn>\"\n","    return {\"text\": prompt}\n","\n","# DataFrame의 각 행에 함수를 적용하여 새로운 데이터셋 생성\n","from datasets import Dataset\n","dataset = Dataset.from_pandas(df)\n","formatted_dataset = dataset.map(create_prompt)"],"metadata":{"id":"M8uJQn9UFmUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =======================================================\n","# 2.2: 변환된 데이터셋 확인\n","# =======================================================\n","print(formatted_dataset[0]['text'])"],"metadata":{"id":"oVS9T5X5FmxR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =======================================================\n","# 3.1: 4비트 양자화 모델 로딩\n","# =======================================================\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","model_id = \"google/gemma-2-9b-it\"\n","new_model_name = \"athena-gemma2-v1\" # 허깅페이스에 저장될 이름\n","\n","# 4비트 양자화 설정\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\" # 자동으로 GPU에 할당\n",")"],"metadata":{"id":"ncP8I1bJF9wS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =======================================================\n","# 3.2: LoRA (PEFT) 설정\n","# =======================================================\n","from peft import LoraConfig\n","\n","# LoRA 설정을 정의\n","peft_config = LoraConfig(\n","    lora_alpha=32,\n","    lora_dropout=0.05,\n","    r=16,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'] # 어텐션 레이어를 타겟으로\n",")"],"metadata":{"id":"a6dYYrprGIUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =======================================================\n","# 4.1: Trainer 설정\n","# =======================================================\n","from transformers import TrainingArguments\n","from trl import SFTTrainer\n","\n","# 훈련 파라미터를 설정\n","training_arguments = TrainingArguments(\n","    output_dir=f\"./{new_model_name}\",\n","    num_train_epochs=3,                     # 3번 반복 학습\n","    per_device_train_batch_size=1,          # 한 번에 1개 데이터씩 처리 (VRAM 절약)\n","    gradient_accumulation_steps=4,          # 4번 그래디언트를 모아서 업데이트\n","    optim=\"paged_adamw_32bit\",\n","    save_steps=25,                          # 25 스텝마다 저장\n","    logging_steps=5,                        # 5 스텝마다 로그 출력\n","    learning_rate=2e-4,\n","    weight_decay=0.001,\n","    fp16=False,\n","    bf16=True,                             # A100/L4 GPU에서 bf16 사용\n","    max_grad_norm=0.3,\n","    max_steps=-1,\n","    warmup_ratio=0.03,\n","    group_by_length=True,\n","    lr_scheduler_type=\"constant\",\n","    report_to=\"tensorboard\"\n",")\n","\n","# SFTTrainer를 설정합니다.\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=formatted_dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=2048,                    # 시퀀스 최대 길이\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n",")"],"metadata":{"id":"oUTEZ1NyGRRp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =======================================================\n","# 4.2: 훈련 시작\n","# =======================================================\n","print(\"파인튜닝 시작\")\n","trainer.train()\n","print(\"파인튜닝 완료!\")"],"metadata":{"id":"_h5kI3QXGReh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# =======================================================\n","# 5.1: 파인튜닝된 모델로 추론 테스트\n","# =======================================================\n","from peft import AutoPeftModelForCausalLM\n","\n","# 테스트할 Instruction\n","test_instruction = df.iloc[0]['Instruction']\n","parts = test_instruction.split('## 분석 데이터:')\n","system_prompt = parts[0].strip()\n","user_content = \"## 분석 데이터:\" + parts[1].strip()\n","test_prompt = f\"<start_of_turn>user\\n{system_prompt}\\n\\n{user_content}<end_of_turn>\\n<start_of_turn>model\\n\"\n","\n","# 파이프라인으로 추론 실행\n","from transformers import pipeline\n","pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=tokenizer, max_length=2048)\n","result = pipe(test_prompt)\n","print(\"===== 파인튜닝된 모델의 답변 =====\")\n","print(result[0]['generated_text'])"],"metadata":{"id":"DzqxVH9GGZIp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4j2e53I-GZbS"},"execution_count":null,"outputs":[]}]}